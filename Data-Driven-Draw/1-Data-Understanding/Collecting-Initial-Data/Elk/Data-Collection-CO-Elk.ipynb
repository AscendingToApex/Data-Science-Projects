{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colorado Elk Tag Application Data Collection\n",
    "\n",
    "Raw Data that was ingested can be found [Here](https://cpw.state.co.us/thingstodo/Pages/Statistics-Elk.aspx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import pdfplumber\n",
    "import os\n",
    "import shutil\n",
    "from io import StringIO\n",
    "from bs4 import BeautifulSoup\n",
    "from re import search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    https://cpw.state.co.us/Documents/Hunting/BigG...\n",
      "1    https://cpw.state.co.us/Documents/Hunting/BigG...\n",
      "2    https://cpw.state.co.us/Documents/Hunting/BigG...\n",
      "3    https://cpw.state.co.us/Documents/Hunting/BigG...\n",
      "4    https://cpw.state.co.us/Documents/Hunting/BigG...\n",
      "5    https://cpw.state.co.us/Documents/Hunting/BigG...\n",
      "6    https://cpw.state.co.us/Documents/Hunting/BigG...\n",
      "7    https://cpw.state.co.us/Documents/Hunting/BigG...\n",
      "Name: URL, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Webscraping URL's Related to Elk Draw Recaps\n",
    "#Scraping html\n",
    "url = 'https://cpw.state.co.us/thingstodo/Pages/Statistics-Elk.aspx'\n",
    "r = requests.get(url)\n",
    "html_doc = r.text\n",
    "soup = BeautifulSoup(html_doc)\n",
    "\n",
    "#isolating href files to a list\n",
    "site_URLs = []\n",
    "for link in soup.find_all('a'):\n",
    "    site_URLs.append((link.get('href')))\n",
    "\n",
    "#Converting to DataFrame and Filtering for information on Draw Recap Statistics\n",
    "elk_URL_DF_Raw = pd.DataFrame(site_URLs,columns =['URL'])\n",
    "elk_URL_DF_Filtered = elk_URL_DF_Raw.loc[elk_URL_DF_Raw['URL'].str.contains('/Documents/Hunting/BigGame/Statistics/ELK',case=False,na=False)]\n",
    "elk_URL_DF_Filtered = elk_URL_DF_Filtered.loc[elk_URL_DF_Raw['URL'].str.contains('ElkDrawRecap.pdf',case=False,na=False)]\n",
    "elk_URL_DF_Filtered['URL'] = 'https://cpw.state.co.us' + elk_URL_DF_Filtered['URL'].astype(str)\n",
    "elk_URL_DF_Filtered.reset_index(inplace = True, drop=True)\n",
    "\n",
    "print(elk_URL_DF_Filtered['URL'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url):\n",
    "    local_filename = url.split('/')[-1]\n",
    "\n",
    "    with requests.get(url) as r:\n",
    "        with open(local_filename, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "        \n",
    "    return local_filename\n",
    "\n",
    "def check_space(string):\n",
    "    \"\"\"Function that returns the number of strings of the inputted string\"\"\"\n",
    "    count = 0\n",
    "    for i in string:\n",
    "        if i == \" \":\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def find_hunt_code(df,page_No,pdf_length):\n",
    "    \"\"\"\n",
    "    Input: Dataframe from the pdf page\n",
    "\n",
    "    Output: Huntcode as a string\n",
    "    \"\"\"\n",
    "    # Utilize a regular expression to find the Hunt Code\n",
    "    Hunt_Code_Search = search('[A-Z]{2}\\d{3}[A-Z]{1}\\d{1}[A-Z]{1}',df.iloc[2,0])\n",
    "\n",
    "    try:\n",
    "        Hunt_Code = Hunt_Code_Search.group(0)\n",
    "        Hunt_Code_Storage = Hunt_Code_Search.group(0)\n",
    "    except:\n",
    "        Hunt_Code = None\n",
    "    \n",
    "    return Hunt_Code\n",
    "\n",
    "def strip_whitespace(df):\n",
    "    \"\"\"\n",
    "    Input: Dataframe from the pdf page\n",
    "    \n",
    "    Output: Dataframe with whitespaces stripped\n",
    "    \"\"\"\n",
    "    # Verifying that the dtypes are objects\n",
    "    df_object = df.select_dtypes(['object'])\n",
    "\n",
    "    #Strip all white spaces\n",
    "    df[df_object.columns] = df_object.apply(lambda x: x.str.strip())\n",
    "\n",
    "    return df\n",
    "\n",
    "def find_preference_point_table(df):\n",
    "    \"\"\"\n",
    "    Input: Dataframe from the pdf page\n",
    "\n",
    "    Output: Preference Point table or Null\n",
    "    \"\"\"\n",
    "    column_name = df.columns[0]\n",
    "\n",
    "    try:\n",
    "        PP_Start = df.loc[df[column_name].str.contains('Choice Preference',na=False,case=False)]\n",
    "        #PP_Start = df.loc[df['                  Colorado Parks and Wildlife   Draw Recap'].str.contains('Choice Preference',na=False,case=False)]\n",
    "        Preference_Points = df.iloc[PP_Start.index[0]+1:]\n",
    "    except:\n",
    "        try:\n",
    "            #There's an extra space between Choice and Preference in some cases\n",
    "            PP_Start = df.loc[df[column_name].str.contains('Choice  Preference',na=False,case=False)]\n",
    "            Preference_Points = df.iloc[PP_Start.index[0]+1:]\n",
    "        except:\n",
    "            #PP_Start = df.loc[df['                  Colorado Parks and Wildlife   Draw Recap'].str.contains('Page',na=False,case=False)]\n",
    "            PP_Start = df.loc[df[column_name].str.contains('Page',na=False,case=False)]\n",
    "            Preference_Points = df.iloc[PP_Start.index[0]+1:]\n",
    "    \n",
    "    return Preference_Points\n",
    "\n",
    "def choice_finder(df):\n",
    "    \"\"\"\n",
    "    Input: Pre processing Preference Points Dataframe\n",
    "    \n",
    "    Output: Isolated Column\n",
    "    \"\"\"\n",
    "    column_name = df.columns[0]\n",
    "\n",
    "    #Handle isolated instances where an extra space was added (i.e. 2020)\n",
    "    df[column_name]= df[column_name].str.replace(\"  \",\" \")\n",
    "\n",
    "    #Check how many spaces are in the strings to see if its in the standardized format, or if an extra character is there\n",
    "    df['Choice Finder'] = df[\"Preference Points Table Buffer\"].apply(lambda x: check_space(x))\n",
    "\n",
    "    #Check if the number of spaces matches the number in the standardized format and the format with the choice included\n",
    "    df = df.loc[(df['Choice Finder'] == 15) | (df['Choice Finder'] == 13)]\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    #Isolate the choice made in the draw, which is indicated based on a string length of 15\n",
    "    df['Choice'] = [x[:1] if y == 15 else None for x,y in zip(df['Preference Points Table Buffer'],df['Choice Finder'])]\n",
    "    #df['Choice'] = [x[:1] if y == 15 else None for x,y in zip(df[column_name],df['Choice Finder'])]\n",
    "    Choice_Index = df[df['Choice'].notnull()].index\n",
    "    df.bfill(axis='rows',inplace=True)\n",
    "    df.ffill(axis='rows',inplace=True)\n",
    "\n",
    "    #Table to merge choices by index after the preference points transpormation is complete\n",
    "    Choice_Merge = df['Choice']\n",
    "\n",
    "    #Restucting rows that had the choice in with the preference points values\n",
    "    df['Preference Points Table Buffer'] = [x[2:] if y == 15 else x for x,y in zip(df['Preference Points Table Buffer'],df['Choice Finder'])]\n",
    "    #df['Preference Points Table Buffer'] = [x[2:] if y == 15 else x for x,y in zip(column_name,df['Choice Finder'])]\n",
    "\n",
    "    return df, Choice_Merge, Choice_Index\n",
    "\n",
    "def preference_points_clean_up(df,Choice_Index):\n",
    "    \"\"\"\n",
    "    Input: Preference Points Buffer 2 and the Choice Index\n",
    "        \n",
    "    Output: Cleaned up preference points table\n",
    "    \"\"\"\n",
    "\n",
    "    #Expand the restructured preference point dataframe, so it matches the format in the pdf\n",
    "    df_Expanded = df.iloc[:,0].str.split(' ',expand=True)\n",
    "\n",
    "    try:\n",
    "        df_Expanded = df_Expanded[[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14]]\n",
    "\n",
    "        #For loop to handle the choice being in the draw portion of the preference point table\n",
    "        for j in range(7,14):\n",
    "            df_Expanded.iloc[Choice_Index,j] = df_Expanded.iloc[Choice_Index,j+1]\n",
    "\n",
    "        #Removes the blank column at the end\n",
    "        del df_Expanded[14]\n",
    "\n",
    "        #Remove the redundent preference point column\n",
    "        del df_Expanded[7]\n",
    "\n",
    "    except:\n",
    "        df_Expanded = df_Expanded[[0,1,2,3,4,5,6,7,8,9,10,11,12,13]]\n",
    "\n",
    "        #Remove the redundent preference point column\n",
    "        del df_Expanded[7]\n",
    "\n",
    "    # Remove any residule components that aren't an integer in the preference points columns\n",
    "    df_Expanded[0] = pd.to_numeric(df_Expanded[0],errors = 'coerce')\n",
    "    df_Expanded.dropna(inplace = True)\n",
    "    df_Expanded[0] = pd.to_numeric(df_Expanded[0],downcast=\"integer\")\n",
    "\n",
    "    #Rename the columns, so they correlate with the pdf format\n",
    "    df_Expanded.columns = ['Preference Points','A-Adult-Res','A-Adult-NonRes','A-Youth-Res','A-Youth-NonRes','A-Landowner(LPP)-Unrestricted' \\\n",
    "    ,'A-Landownder(LPP)-Restricted','D-Adult-Res','D-Adult-NonRes','D-Youth-Res','D-Youth-NonRes'\n",
    "    ,'D-Landowner(LPP)-Unrestricted','D-Landownder(LPP)-Restricted']\n",
    "\n",
    "    return df_Expanded\n",
    "\n",
    "def preference_points_finalize(df, Choice_Merge, Hunt_Code, Draw_Year):\n",
    "    \"\"\"\n",
    "    Input: Preference Points Expanded DataFrame, Choice Merge Dataframe, Hunt_Code, and Draw Year\n",
    "\n",
    "    Output: Applicants and Drew Dataframes accordingly\n",
    "    \"\"\"\n",
    "    #Isolate the columns appliable to the draw applicants\n",
    "    Applicant_Preference_Points_Buffer = df[['Preference Points','A-Adult-Res','A-Adult-NonRes','A-Youth-Res','A-Youth-NonRes','A-Landowner(LPP)-Unrestricted' \\\n",
    "    ,'A-Landownder(LPP)-Restricted']]\n",
    "\n",
    "    #Merge the Choice on index; creating the applicant and successful draw dataframe\n",
    "    Applicant_Preference_Points = Applicant_Preference_Points_Buffer.merge(Choice_Merge, left_index=True, right_index=True)\n",
    "\n",
    "    Draw_Preference_Points_Buffer = df[['Preference Points','D-Adult-Res','D-Adult-NonRes','D-Youth-Res','D-Youth-NonRes'\n",
    "    ,'D-Landowner(LPP)-Unrestricted','D-Landownder(LPP)-Restricted']]\n",
    "\n",
    "    Draw_Preference_Points = Draw_Preference_Points_Buffer.merge(Choice_Merge, left_index=True,right_index=True)\n",
    "\n",
    "    #Add the Hunt Code and Draw year to the dataframes\n",
    "    Applicant_Preference_Points['Hunt Code'] = Hunt_Code\n",
    "    Applicant_Preference_Points['Year'] = Draw_Year\n",
    "    Draw_Preference_Points['Hunt Code'] = Hunt_Code\n",
    "    Draw_Preference_Points['Year'] = Draw_Year\n",
    "\n",
    "    #Create a Primary Key for the dataframe\n",
    "    Applicant_Preference_Points['Hunt Key'] = [x + '-' + y + '-' + str(z) for x,y,z in zip(Applicant_Preference_Points['Hunt Code'], \\\n",
    "        Applicant_Preference_Points['Year'],Applicant_Preference_Points['Preference Points'])]\n",
    "    Draw_Preference_Points['Hunt Key'] = [x + '-' + y + '-' + str(z) for x,y,z in zip(Draw_Preference_Points['Hunt Code'],Draw_Preference_Points['Year'] \\\n",
    "        ,Draw_Preference_Points['Preference Points'])]\n",
    "\n",
    "    return Applicant_Preference_Points, Draw_Preference_Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://cpw.state.co.us/Documents/Hunting/BigGame/Statistics/Elk/2015ElkDrawRecap.pdf'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elk_URL_DF_Filtered['URL'][7]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF Scraping Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(0,len(elk_URL_DF_Filtered)):\n",
    "for i in range(7,8):\n",
    "\n",
    "    stats_url = download_file(elk_URL_DF_Filtered['URL'][i])\n",
    "\n",
    "    # Instanciate empty dataframes for the consolidated output of processed pages\n",
    "    All_Applicant_Preference_Points = pd.DataFrame()\n",
    "    All_Draw_Preference_Points = pd.DataFrame()\n",
    "    Pages_with_issues = []\n",
    "\n",
    "    with pdfplumber.open(stats_url) as pdf:\n",
    "        number_of_pages = len(pdf.pages)\n",
    "        for j in range(2,number_of_pages):\n",
    "            # Export the pdf page's raw text as a dataframe\n",
    "            page = pdf.pages[j]\n",
    "            text = page.extract_text()\n",
    "            df = pd.read_csv(StringIO(text))\n",
    "\n",
    "            #Length of less than 8 skips pages that only have the portions of the summary table below the preference point table\n",
    "            if len(df) > 8:\n",
    "                try:\n",
    "                    #Find the Hunt Code on the page and leave it the same if there isn't one on the page\n",
    "                    Hunt_Code_Buffer = find_hunt_code(df,j,number_of_pages)\n",
    "                    Hunt_Code = Hunt_Code if Hunt_Code_Buffer is None else Hunt_Code_Buffer\n",
    "\n",
    "                    #print('Hunt Code Found Successfully...')\n",
    "\n",
    "                    # Utilize a regular expression to find the Year of the Draw Recap\n",
    "                    Draw_Year_Search = search('\\d{4}',df.iloc[0,0])\n",
    "                    Draw_Year = Draw_Year_Search.group(0)\n",
    "\n",
    "                    #print('Year Found Successfully...')\n",
    "\n",
    "                    #Strip whitespaces\n",
    "                    df_stripped = strip_whitespace(df)\n",
    "\n",
    "                    #print('Whitespaces Stripped Successfully..')\n",
    "\n",
    "                    #Isolating where the preference points portion of the dataframe starts\n",
    "                    Preference_Points_Buffer = find_preference_point_table(df_stripped)\n",
    "\n",
    "                    #print('Preference Point Table Found Successfully...')\n",
    "\n",
    "                    Preference_Points_Buffer.reset_index(inplace=True, drop=True)\n",
    "                    Preference_Points_Buffer.columns = [\"Preference Points Table Buffer\"]\n",
    "\n",
    "                    #print('Index reset and column renamed successfully...')\n",
    "\n",
    "                    #Isolate the Choice and reformat to a standardize format for separating the preference points\n",
    "                    Preference_Points_Buffer2, Choice_Merge, Choice_Index = choice_finder(Preference_Points_Buffer)\n",
    "\n",
    "                    #print('Choice found successfully...')\n",
    "\n",
    "                    # Reformat Preference Point DataFrame, so it's easier to interpret\n",
    "                    Preference_Points_Expanded = preference_points_clean_up(Preference_Points_Buffer2, Choice_Index)\n",
    "\n",
    "                    #print('Preference Point table cleaned up successfully...')\n",
    "\n",
    "                    #Perform final clean-up to and segregation\n",
    "                    Applicant_Preference_Points, Draw_Preference_Points = preference_points_finalize(Preference_Points_Expanded, Choice_Merge, Hunt_Code, Draw_Year)\n",
    "\n",
    "                    #print('Preference Point Clean-up Completed Successfully')\n",
    "\n",
    "                    #Append to a generalized Dataframe for multiple pages processed\n",
    "                    All_Applicant_Preference_Points = All_Applicant_Preference_Points.append(Applicant_Preference_Points)\n",
    "                    All_Draw_Preference_Points = All_Draw_Preference_Points.append(Draw_Preference_Points)\n",
    "\n",
    "                    #print('Appended to consolidated dataframe successfully')\n",
    "                except:\n",
    "                    Pages_with_issues.append(j)\n",
    "                    continue\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    All_Applicant_Preference_Points.to_excel('Output-Data\\\\Applicant-Data\\\\Dirty\\\\'+ Draw_Year+'-All-Applicant-Preference-Points.xlsx')\n",
    "    All_Draw_Preference_Points.to_excel('Output-Data\\\\Draw-Data\\\\Dirty\\\\'+Draw_Year+'-All-Draw-Preference-Points.xlsx')\n",
    "    \n",
    "    Pages_With_Issues_DF = pd.DataFrame(Pages_with_issues, columns=['Pages'])\n",
    "    if len(Pages_With_Issues_DF) > 0:\n",
    "        Pages_With_Issues_DF.to_excel('Output-Data\\\\Pages-with-Issues\\\\'+ Draw_Year+'-Pages-With-Issues.xlsx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Data Directory Clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current directory\n",
    "directory = os.getcwd()\n",
    "    \n",
    "#Iterate over the directory looking for the downloaded input pdf documents and put into a separate directory\n",
    "for file in os.listdir(directory):\n",
    "     filename = os.fsdecode(file)\n",
    "     if filename.endswith(\".pdf\"): \n",
    "        original = filename\n",
    "        target = 'Input-Data\\\\'+filename\n",
    "\n",
    "        shutil.move(original, target)\n",
    "        continue\n",
    "     else:\n",
    "         continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up data types and Choice column for applicants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set directory to iterate throughout all the the files in\n",
    "directory = 'Output-Data\\\\Applicant-Data\\\\Dirty\\\\'\n",
    "    \n",
    "#Iterate over all the files in the directory and perfom transformations if it's an .xlsx file\n",
    "for file in os.listdir(directory):\n",
    "     filename = os.fsdecode(file)\n",
    "     if (filename.endswith(\".xlsx\")) & (filename[0:3] != \"Tot\"):\n",
    "      \n",
    "      # Import File as a dataframe\n",
    "      df = pd.read_excel(directory+file)\n",
    "\n",
    "      # Replace the \"-\" placeholder used by CPW with 0\n",
    "      df = df.replace(\"-\",0)\n",
    "\n",
    "      # Dictionary to map the columns as the correct data type\n",
    "      convert_dict ={'Preference Points':int\n",
    "      ,'A-Adult-Res':int\n",
    "      ,'A-Adult-NonRes':int\n",
    "      ,'A-Youth-Res':int\n",
    "      ,'A-Youth-NonRes':int\n",
    "      ,'A-Landowner(LPP)-Unrestricted':int\n",
    "      ,'A-Landownder(LPP)-Restricted':int}\n",
    "\n",
    "      # Remap dataframe with the correct data type\n",
    "      df= df.astype(convert_dict)\n",
    "\n",
    "      #Loop to check the preference points and hunt code, then assign the correct Choice\n",
    "      Choice = 1\n",
    "      for i in range (1,len(df)):\n",
    "         if df.iloc[i,9] == df.iloc[i-1,9]:\n",
    "            if df.iloc[i,0] == df.iloc[i-1,0]+1:\n",
    "               df.iloc[i,8] = Choice\n",
    "            else:\n",
    "               Choice +=1\n",
    "               df.iloc[i,8] = Choice\n",
    "         else:\n",
    "            Choice = 1\n",
    "            df.iloc[i,8] = Choice\n",
    "\n",
    "      df['Choice']=df['Choice'].fillna(0).astype(int)\n",
    "\n",
    "      #Delete old Choice column\n",
    "      del df[df.columns[0]]\n",
    "\n",
    "      #Select only Choice 1 applicants\n",
    "      output_df  = df.loc[df['Choice']==1]\n",
    "\n",
    "      # Output the re-formatted dataframe\n",
    "      output_df.to_excel('Output-Data\\\\Applicant-Data\\\\Cleaned\\\\'+filename)\n",
    "\n",
    "      continue\n",
    "     else:\n",
    "      continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Cleaning up data types and choice column for draws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set directory to iterate throughout all the the files in\n",
    "directory = 'Output-Data\\\\Draw-Data\\\\Dirty\\\\'\n",
    "    \n",
    "#Iterate over all the files in the directory and perfom transformations if it's an .xlsx file    \n",
    "for file in os.listdir(directory):\n",
    "     filename = os.fsdecode(file)\n",
    "     if (filename.endswith(\".xlsx\")) & (filename[0:3] != \"Tot\"):\n",
    "\n",
    "      # Import File as a dataframe\n",
    "      df = pd.read_excel(directory+file)\n",
    "\n",
    "      # Replace the \"-\" placeholder used by CPW with 0\n",
    "      df = df.replace(\"-\",0)\n",
    "\n",
    "      # Dictionary to map the columns as the correct data type\n",
    "      convert_dict ={'Preference Points':int\n",
    "      ,'D-Adult-Res':int\n",
    "      ,'D-Adult-NonRes':int\n",
    "      ,'D-Youth-Res':int\n",
    "      ,'D-Youth-NonRes':int\n",
    "      ,'D-Landowner(LPP)-Unrestricted':int\n",
    "      ,'D-Landownder(LPP)-Restricted':int}\n",
    "\n",
    "      # Remap dataframe with the correct data type\n",
    "      df= df.astype(convert_dict)\n",
    "\n",
    "      #Loop to check the preference points and hunt code, then assign the correct Choice\n",
    "      Choice = 1\n",
    "      for i in range (1,len(df)):\n",
    "         if df.iloc[i,9] == df.iloc[i-1,9]:\n",
    "            if df.iloc[i,0] == df.iloc[i-1,0]+1:\n",
    "               df.iloc[i,8] = Choice\n",
    "            else:\n",
    "               Choice +=1\n",
    "               df.iloc[i,8] = Choice\n",
    "         else:\n",
    "            Choice = 1\n",
    "            df.iloc[i,8] = Choice\n",
    "\n",
    "      df['Choice']=df['Choice'].fillna(0).astype(int)\n",
    "\n",
    "      #Delete old Choice column\n",
    "      del df[df.columns[0]]\n",
    "\n",
    "      #Select only Choice 1 applicants\n",
    "      output_df  = df.loc[df['Choice']==1]\n",
    "\n",
    "      # Output the re-formatted dataframe\n",
    "      output_df.to_excel('Output-Data\\\\Draw-Data\\\\Cleaned\\\\'+filename)\n",
    "\n",
    "      continue\n",
    "     else:\n",
    "      continue"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the total tags awarded by year, hunt code, and resident type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set directory to iterate throughout all the the files in\n",
    "directory = 'Output-Data\\\\Draw-Data\\\\Cleaned\\\\'\n",
    "    \n",
    "#Iterate over all the files in the directory and perfom transformations if it's an .xlsx file\n",
    "for file in os.listdir(directory):\n",
    "    filename = os.fsdecode(file)\n",
    "    if (filename.endswith(\".xlsx\")) & (filename[0:3] != \"Tot\"):\n",
    "        df_total_tags_init = pd.read_excel(directory+file)\n",
    "\n",
    "        #Dictionary to add how many tags were awarded for each sub category\n",
    "        Total_Tags_Dict = {}\n",
    "        for i in range (0,len(df_total_tags_init)):\n",
    "            if i == 0:\n",
    "                AR_Total_Tags_Given = df_total_tags_init.iloc[i,2]\n",
    "                ANR_Total_Tags_Given = df_total_tags_init.iloc[i,3]\n",
    "                YR_Total_Tags_Given = df_total_tags_init.iloc[i,4]\n",
    "                YNR_Total_Tags_Given = df_total_tags_init.iloc[i,5]\n",
    "                LOU_Total_Tags_Given = df_total_tags_init.iloc[i,6]\n",
    "                LOR_Total_Tags_Given = df_total_tags_init.iloc[i,7]\n",
    "            elif df_total_tags_init.iloc[i,9] == df_total_tags_init.iloc[i-1,9]:\n",
    "                AR_Total_Tags_Given += df_total_tags_init.iloc[i,2]\n",
    "                ANR_Total_Tags_Given += df_total_tags_init.iloc[i,3]\n",
    "                YR_Total_Tags_Given += df_total_tags_init.iloc[i,4]\n",
    "                YNR_Total_Tags_Given += df_total_tags_init.iloc[i,5]\n",
    "                LOU_Total_Tags_Given += df_total_tags_init.iloc[i,6]\n",
    "                LOR_Total_Tags_Given += df_total_tags_init.iloc[i,7]\n",
    "            else:\n",
    "                Total_Tags_Dict[df_total_tags_init.iloc[i-1,9]] = {\n",
    "                    'Adult-Res':AR_Total_Tags_Given\n",
    "                    ,'Adult-Non Res':ANR_Total_Tags_Given\n",
    "                    ,'Youth-Res':YR_Total_Tags_Given\n",
    "                    ,'Youth-Non Res':YNR_Total_Tags_Given\n",
    "                    ,'Landowner-Unrestricted':LOU_Total_Tags_Given\n",
    "                    ,'Landownder-Restricted':LOR_Total_Tags_Given\n",
    "                }\n",
    "                AR_Total_Tags_Given = df_total_tags_init.iloc[i,2]\n",
    "                ANR_Total_Tags_Given = df_total_tags_init.iloc[i,3]\n",
    "                YR_Total_Tags_Given = df_total_tags_init.iloc[i,4]\n",
    "                YNR_Total_Tags_Given = df_total_tags_init.iloc[i,5]\n",
    "                LOU_Total_Tags_Given = df_total_tags_init.iloc[i,6]\n",
    "                LOR_Total_Tags_Given = df_total_tags_init.iloc[i,7]\n",
    "\n",
    "        # Convert Dictionary to a dataframe\n",
    "        Total_Tags_Awarded =pd.DataFrame.from_dict(Total_Tags_Dict,orient='index')\n",
    "\n",
    "        # Sum each column to get the total number of tags awarded\n",
    "        Total_Tags_Awarded['Total Tags'] = [a+b+c+d+e+f for a,b,c,d,e,f in zip(Total_Tags_Awarded['Adult-Res'] \\\n",
    "            ,Total_Tags_Awarded['Adult-Non Res']\n",
    "            ,Total_Tags_Awarded['Youth-Res']\n",
    "            ,Total_Tags_Awarded['Youth-Non Res']\n",
    "            ,Total_Tags_Awarded['Landowner-Unrestricted']\n",
    "            ,Total_Tags_Awarded['Landownder-Restricted'])]\n",
    "        \n",
    "        # Create a Year Column based on the filename string\n",
    "        Total_Tags_Awarded['Year'] = filename[0:4]\n",
    "\n",
    "        # Create a Hunt Code Column\n",
    "        Total_Tags_Awarded['Hunt Code'] = Total_Tags_Awarded.index\n",
    "\n",
    "        #Create a column for a the primary key\n",
    "        Total_Tags_Awarded['Primary Key'] = [x +'-'+ y for x,y in zip(Total_Tags_Awarded['Year'], Total_Tags_Awarded.index)]\n",
    "\n",
    "        #Output the Total Tags awarded dataframe\n",
    "        Total_Tags_Awarded.to_excel('Output-Data\\\\Total-Awarded\\\\'+filename[0:4]+'-Total-Tags-Awarded.xlsx')\n",
    "        continue\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the total quantity of applicants by year, hunt code, and resident type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set directory to iterate throughout all the the files in\n",
    "directory = 'Output-Data\\\\Applicant-Data\\\\Cleaned\\\\'\n",
    "    \n",
    "#Iterate over all the files in the directory and perfom transformations if it's an .xlsx file\n",
    "for file in os.listdir(directory):\n",
    "    filename = os.fsdecode(file)\n",
    "    if (filename.endswith(\".xlsx\")) & (filename[0:3] != \"Tot\"):\n",
    "        df_total_apps_init = pd.read_excel(directory+file)\n",
    "\n",
    "        #Dictionary to add how many tags were awarded for each sub category\n",
    "        Total_Apps_Dict = {}\n",
    "        for i in range (0,len(df_total_apps_init)):\n",
    "            if i == 0:\n",
    "                AR_Total_Apps = df_total_apps_init.iloc[i,2]\n",
    "                ANR_Total_Apps = df_total_apps_init.iloc[i,3]\n",
    "                YR_Total_Apps = df_total_apps_init.iloc[i,4]\n",
    "                YNR_Total_Apps = df_total_apps_init.iloc[i,5]\n",
    "                LOU_Total_Apps = df_total_apps_init.iloc[i,6]\n",
    "                LOR_Total_Apps = df_total_apps_init.iloc[i,7]\n",
    "            elif df_total_tags_init.iloc[i,9] == df_total_tags_init.iloc[i-1,9]:\n",
    "                AR_Total_Apps += df_total_apps_init.iloc[i,2]\n",
    "                ANR_Total_Apps += df_total_apps_init.iloc[i,3]\n",
    "                YR_Total_Apps += df_total_apps_init.iloc[i,4]\n",
    "                YNR_Total_Apps += df_total_apps_init.iloc[i,5]\n",
    "                LOU_Total_Apps += df_total_apps_init.iloc[i,6]\n",
    "                LOR_Total_Apps += df_total_apps_init.iloc[i,7]\n",
    "            else:\n",
    "                Total_Apps_Dict[df_total_apps_init.iloc[i-1,9]] = {\n",
    "                    'Adult-Res':AR_Total_Apps\n",
    "                    ,'Adult-Non Res':ANR_Total_Apps\n",
    "                    ,'Youth-Res':YR_Total_Apps\n",
    "                    ,'Youth-Non Res':YNR_Total_Apps\n",
    "                    ,'Landowner-Unrestricted':LOU_Total_Apps\n",
    "                    ,'Landownder-Restricted':LOR_Total_Apps\n",
    "                }\n",
    "                AR_Total_Apps = df_total_apps_init.iloc[i,2]\n",
    "                ANR_Total_Apps = df_total_apps_init.iloc[i,3]\n",
    "                YR_Total_Apps = df_total_apps_init.iloc[i,4]\n",
    "                YNR_Total_Apps = df_total_apps_init.iloc[i,5]\n",
    "                LOU_Total_Apps = df_total_apps_init.iloc[i,6]\n",
    "                LOR_Total_Apps = df_total_apps_init.iloc[i,7]\n",
    "\n",
    "        # Convert Dictionary to a dataframe\n",
    "        Total_Apps =pd.DataFrame.from_dict(Total_Apps_Dict,orient='index')\n",
    "\n",
    "        # Sum each column to get the total number of tags awarded\n",
    "        Total_Apps['Total Applications'] = [a+b+c+d+e+f for a,b,c,d,e,f in zip(Total_Apps['Adult-Res'] \\\n",
    "            ,Total_Apps['Adult-Non Res']\n",
    "            ,Total_Apps['Youth-Res']\n",
    "            ,Total_Apps['Youth-Non Res']\n",
    "            ,Total_Apps['Landowner-Unrestricted']\n",
    "            ,Total_Apps['Landownder-Restricted'])]\n",
    "        \n",
    "        # Create a Year Column based on the filename string\n",
    "        Total_Apps['Year'] = filename[0:4]\n",
    "\n",
    "        # Create a Hunt Code Column\n",
    "        Total_Apps['Hunt Code'] = Total_Apps.index\n",
    "\n",
    "        #Create a column for a the primary key\n",
    "        Total_Apps['Primary Key'] = [x +'-'+ y for x,y in zip(Total_Apps['Year'], Total_Apps.index)]\n",
    "\n",
    "        #Output the Total Tags awarded dataframe\n",
    "        Total_Apps.to_excel('Output-Data\\\\Total-Applied\\\\'+filename[0:4]+'-Total-Applied.xlsx')\n",
    "        continue\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the minimum preference points to draw tags for hunt codes and resident type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'Output-Data\\\\Draw-Data\\\\Cleaned\\\\'\n",
    "    \n",
    "#Iterate over all the files in the directory and perfom transformations if it's an .xlsx file\n",
    "for file in os.listdir(directory):\n",
    "    filename = os.fsdecode(file)\n",
    "    if (filename.endswith(\".xlsx\")) & (filename[0:3] != \"Tot\"):\n",
    "        df_min_pts_init = pd.read_excel(directory+file)\n",
    "\n",
    "        #Dictionary to add how many tags were awarded for each sub category\n",
    "        Min_Pref_Dict = {}\n",
    "        for i in range (0,len(df_min_pts_init)):\n",
    "            if i == 0:\n",
    "                if (df_min_pts_init.iloc[i,2] > 0):\n",
    "                    AR_Min_Pts = df_min_pts_init.iloc[i,1]\n",
    "                else:\n",
    "                    AR_Min_Pts = 99\n",
    "                \n",
    "                if (df_min_pts_init.iloc[i,3] > 0):\n",
    "                    ANR_Min_Pts = df_min_pts_init.iloc[i,1]\n",
    "                else:\n",
    "                    ANR_Min_Pts = 99\n",
    "\n",
    "                if (df_min_pts_init.iloc[i,4] > 0):\n",
    "                    YR_Min_Pts = df_min_pts_init.iloc[i,1]\n",
    "                else:\n",
    "                    YR_Min_Pts = 99\n",
    "\n",
    "                if (df_min_pts_init.iloc[i,5] > 0):\n",
    "                    YNR_Min_Pts = df_min_pts_init.iloc[i,1]\n",
    "                else:\n",
    "                    YNR_Min_Pts = 99\n",
    "\n",
    "                if (df_min_pts_init.iloc[i,6] > 0):\n",
    "                    LOU_Min_Pts = df_min_pts_init.iloc[i,1]\n",
    "                else:\n",
    "                    LOU_Min_Pts = 99\n",
    "\n",
    "                if (df_min_pts_init.iloc[i,7] > 0):\n",
    "                    LOR_Min_Pts = df_min_pts_init.iloc[i,1]\n",
    "                else:\n",
    "                    LOR_Min_Pts = 99\n",
    "\n",
    "            elif (df_min_pts_init.iloc[i,9] == df_min_pts_init.iloc[i-1,9]):\n",
    "                if ((df_min_pts_init.iloc[i,2] > 0) & (AR_Min_Pts == 99)):\n",
    "                    AR_Min_Pts = df_min_pts_init.iloc[i,1]\n",
    "                elif ((df_min_pts_init.iloc[i,2] > 0) & (df_min_pts_init.iloc[i,1] < AR_Min_Pts)):\n",
    "                    AR_Min_Pts = df_min_pts_init.iloc[i,1]\n",
    "                \n",
    "                if ((df_min_pts_init.iloc[i,3] > 0) & (ANR_Min_Pts == 99)):\n",
    "                    ANR_Min_Pts = df_min_pts_init.iloc[i,1]\n",
    "                elif ((df_min_pts_init.iloc[i,3] > 0) & (df_min_pts_init.iloc[i,1] < ANR_Min_Pts)):\n",
    "                    ANR_Min_Pts = df_min_pts_init.iloc[i,1]\n",
    "\n",
    "                if ((df_min_pts_init.iloc[i,4] > 0) & (YR_Min_Pts == 99)):\n",
    "                    YR_Min_Pts = df_min_pts_init.iloc[i,1]\n",
    "                elif ((df_min_pts_init.iloc[i,4] > 0) & (df_min_pts_init.iloc[i,1] < YR_Min_Pts)):\n",
    "                    YR_Min_Pts = df_min_pts_init.iloc[i,1]\n",
    "\n",
    "                if ((df_min_pts_init.iloc[i,5] > 0) & (YNR_Min_Pts == 99)):\n",
    "                    YNR_Min_Pts = df_min_pts_init.iloc[i,1]\n",
    "                elif ((df_min_pts_init.iloc[i,5] > 0) & (df_min_pts_init.iloc[i,1] < YNR_Min_Pts)):\n",
    "                    YNR_Min_Pts = df_min_pts_init.iloc[i,1]\n",
    "\n",
    "                if ((df_min_pts_init.iloc[i,6] > 0) & (LOU_Min_Pts == 99)):\n",
    "                    LOU_Min_Pts = df_min_pts_init.iloc[i,1]\n",
    "                elif ((df_min_pts_init.iloc[i,6] > 0) & (df_min_pts_init.iloc[i,1] < LOU_Min_Pts)):\n",
    "                    LOU_Min_Pts = df_min_pts_init.iloc[i,1]\n",
    "\n",
    "                if ((df_min_pts_init.iloc[i,7] > 0) & (LOR_Min_Pts == 99)):\n",
    "                    LOR_Min_Pts = df_min_pts_init.iloc[i,1]\n",
    "                elif ((df_min_pts_init.iloc[i,7] > 0) & (df_min_pts_init.iloc[i,1] < LOR_Min_Pts)):\n",
    "                    LOR_Min_Pts = df_min_pts_init.iloc[i,1]\n",
    "\n",
    "            else:\n",
    "                Min_Pref_Dict[df_min_pts_init.iloc[i-1,9]] = {\n",
    "                    'Adult-Res':AR_Min_Pts\n",
    "                    ,'Adult-Non Res':ANR_Min_Pts\n",
    "                    ,'Youth-Res':YR_Min_Pts\n",
    "                    ,'Youth-Non Res':YNR_Min_Pts\n",
    "                    ,'Landowner-Unrestricted':LOU_Min_Pts\n",
    "                    ,'Landownder-Restricted':LOR_Min_Pts\n",
    "                }\n",
    "                if (df_min_pts_init.iloc[i,2] > 0):\n",
    "                    AR_Min_Pts = df_min_pts_init.iloc[i,1]\n",
    "                else:\n",
    "                    AR_Min_Pts = 99\n",
    "                \n",
    "                if (df_min_pts_init.iloc[i,3] > 0):\n",
    "                    ANR_Min_Pts = df_min_pts_init.iloc[i,1]\n",
    "                else:\n",
    "                    ANR_Min_Pts = 99\n",
    "\n",
    "                if (df_min_pts_init.iloc[i,4] > 0):\n",
    "                    YR_Min_Pts = df_min_pts_init.iloc[i,1]\n",
    "                else:\n",
    "                    YR_Min_Pts = 99\n",
    "\n",
    "                if (df_min_pts_init.iloc[i,5] > 0):\n",
    "                    YNR_Min_Pts = df_min_pts_init.iloc[i,1]\n",
    "                else:\n",
    "                    YNR_Min_Pts = 99\n",
    "\n",
    "                if (df_min_pts_init.iloc[i,6] > 0):\n",
    "                    LOU_Min_Pts = df_min_pts_init.iloc[i,1]\n",
    "                else:\n",
    "                    LOU_Min_Pts = 99\n",
    "\n",
    "                if (df_min_pts_init.iloc[i,7] > 0):\n",
    "                    LOR_Min_Pts = df_min_pts_init.iloc[i,1]\n",
    "                else:\n",
    "                    LOR_Min_Pts = 99\n",
    "\n",
    "        # Convert Dictionary to a dataframe\n",
    "        Total_Min_Pref_Awarded =pd.DataFrame.from_dict(Min_Pref_Dict,orient='index')\n",
    "\n",
    "        # Create a Year Column based on the filename string\n",
    "        Total_Min_Pref_Awarded['Year'] = filename[0:4]\n",
    "\n",
    "        #Create a column for a the primary key\n",
    "        Total_Min_Pref_Awarded['Primary Key'] = [x +'-'+ y for x,y in zip(Total_Min_Pref_Awarded['Year'], Total_Min_Pref_Awarded.index)]\n",
    "\n",
    "        # Replace the 99 placeholder with None\n",
    "        Total_Min_Pref_Awarded['Adult-Res'].replace({99: None},inplace=True)\n",
    "        Total_Min_Pref_Awarded['Adult-Non Res'].replace({99: None},inplace=True)\n",
    "        Total_Min_Pref_Awarded['Youth-Res'].replace({99: None},inplace=True)\n",
    "        Total_Min_Pref_Awarded['Youth-Non Res'].replace({99: None},inplace=True)\n",
    "        Total_Min_Pref_Awarded['Landowner-Unrestricted'].replace({99: None},inplace=True)\n",
    "        Total_Min_Pref_Awarded['Landownder-Restricted'].replace({99: None},inplace=True)\n",
    "\n",
    "        #Output the Total Tags awarded dataframe\n",
    "        Total_Min_Pref_Awarded.to_excel('Output-Data\\\\Minimum-Preference-Points\\\\'+filename[0:4]+'-Min-Pref-Points.xlsx')\n",
    "        continue\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consolidate Data for each output into a master source for all date ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'Output-Data\\\\Applicant-Data\\\\Cleaned\\\\'\n",
    "\n",
    "All_Applicants = pd.DataFrame()\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    filename = os.fsdecode(file)\n",
    "    if (filename.endswith(\".xlsx\")) & (filename[0:3] != \"Tot\"):\n",
    "        df_Applicants = pd.read_excel(directory+file)\n",
    "        All_Applicants = pd.concat([All_Applicants,df_Applicants])\n",
    "        continue\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "del All_Applicants[All_Applicants.columns[0]]\n",
    "\n",
    "All_Applicants['Secondary Key'] = [str(x) + \"-\" + y for x,y in zip(All_Applicants['Year'],All_Applicants['Hunt Code'])]\n",
    "\n",
    "All_Applicants.to_excel('Output-Data\\\\Applicant-Data\\\\Cleaned\\\\Total-All-Applicant-Preference_Points.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'Output-Data\\\\Draw-Data\\\\Cleaned\\\\'\n",
    "\n",
    "All_Draw = pd.DataFrame()\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    filename = os.fsdecode(file)\n",
    "    if (filename.endswith(\".xlsx\")) & (filename[0:3] != \"Tot\"):\n",
    "        df_Draw = pd.read_excel(directory+file)\n",
    "        All_Draw = pd.concat([All_Draw,df_Draw])\n",
    "        continue\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "del All_Draw[All_Draw.columns[0]]\n",
    "\n",
    "All_Draw['Secondary Key'] = [str(x) + \"-\" + y for x,y in zip(All_Draw['Year'],All_Draw['Hunt Code'])]\n",
    "\n",
    "All_Draw.to_excel('Output-Data\\\\Draw-Data\\\\Cleaned\\\\Total-All-Draw-Preference_Points.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'Output-Data\\\\Total-Awarded\\\\'\n",
    "\n",
    "All_Awarded = pd.DataFrame()\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    filename = os.fsdecode(file)\n",
    "    if (filename.endswith(\".xlsx\")) & (filename[0:3] != \"All\"):\n",
    "        df_Total_Awarded = pd.read_excel(directory+file)\n",
    "        All_Awarded = pd.concat([All_Awarded,df_Total_Awarded])\n",
    "        continue\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "del All_Awarded[All_Awarded.columns[0]]\n",
    "\n",
    "All_Awarded.to_excel('Output-Data\\\\Total-Awarded\\\\All-Total-Tags-Awarded.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'Output-Data\\\\Total-Applied\\\\'\n",
    "\n",
    "All_Applied = pd.DataFrame()\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    filename = os.fsdecode(file)\n",
    "    if (filename.endswith(\".xlsx\")) & (filename[0:3] != \"All\"):\n",
    "        df_Total_Applied = pd.read_excel(directory+file)\n",
    "        All_Applied = pd.concat([All_Applied,df_Total_Applied])\n",
    "        continue\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "del All_Applied[All_Applied.columns[0]]\n",
    "\n",
    "All_Applied.to_excel('Output-Data\\\\Total-Applied\\\\All-Total-Applied.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'Output-Data\\\\Minimum-Preference-Points\\\\'\n",
    "\n",
    "All_Min_Pref = pd.DataFrame()\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    filename = os.fsdecode(file)\n",
    "    if (filename.endswith(\".xlsx\")) & (filename[0:3] != \"All\"):\n",
    "        df_All_Min_Pref_Points = pd.read_excel(directory+file)\n",
    "        df_All_Min_Pref_Points.reset_index(inplace=False,drop=False)\n",
    "        All_Min_Pref = pd.concat([All_Min_Pref,df_All_Min_Pref_Points])\n",
    "        All_Min_Pref.reset_index(inplace=True, drop=True)\n",
    "        continue\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "All_Min_Pref.rename(columns = {'Unnamed: 0':\"Hunt Code\"},inplace=True)\n",
    "\n",
    "All_Min_Pref['Hunt Code'] = [x if not pd.isnull(x) else y for x,y in zip(All_Min_Pref['Hunt Code'],All_Min_Pref['Column1'])]\n",
    "\n",
    "del All_Min_Pref['Column1']\n",
    "\n",
    "All_Min_Pref.to_excel('Output-Data\\\\Minimum-Preference-Points\\\\All-Min-Pref-Points.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "92eeafcbabfc37afade1f0f453883f240313697269fbef933ea65d1be2634430"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
